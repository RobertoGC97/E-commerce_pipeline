# ğŸ›’ Olist Data Pipeline - Data Engineering Project

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![PySpark](https://img.shields.io/badge/PySpark-3.5.0-orange.svg)
![Pandas](https://img.shields.io/badge/Pandas-2.1.0-green.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

Pipeline completo de ETL (Extract, Transform, Load) para anÃ¡lisis de datos de e-commerce utilizando el dataset pÃºblico de **Olist** (Brasil). Este proyecto demuestra habilidades en ingenierÃ­a de datos, programaciÃ³n orientada a objetos con Python, procesamiento distribuido con Spark y anÃ¡lisis SQL avanzado.

---

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n](#-descripciÃ³n)
- [Stack TecnolÃ³gico](#-stack-tecnolÃ³gico)
- [Arquitectura](#-arquitectura)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Uso](#-uso)
- [CaracterÃ­sticas Principales](#-caracterÃ­sticas-principales)
- [Queries SQL](#-queries-sql)
- [Resultados](#-resultados)
- [PrÃ³ximas Mejoras](#-prÃ³ximas-mejoras)
- [Autor](#-autor)

---

## ğŸ¯ DescripciÃ³n

Este proyecto implementa un pipeline de datos completo que procesa informaciÃ³n de mÃ¡s de 100,000 Ã³rdenes del marketplace brasileÃ±o Olist. El pipeline extrae datos de mÃºltiples fuentes CSV, aplica transformaciones y limpieza, y carga los resultados en una base de datos PostgreSQL, generando reportes analÃ­ticos en el proceso.

**Dataset utilizado:** [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

### Objetivos del Proyecto

âœ… Demostrar arquitectura ETL escalable con Python  
âœ… Aplicar principios de ProgramaciÃ³n Orientada a Objetos  
âœ… Implementar procesamiento distribuido con PySpark  
âœ… Ejecutar anÃ¡lisis SQL avanzado (JOINS, agregaciones, CTEs)  
âœ… Crear pipeline reproducible con buenas prÃ¡cticas de cÃ³digo

---

## ğŸ›  Stack TecnolÃ³gico

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|-----------|---------|-----------|
| **Python** | 3.9+ | Lenguaje principal |
| **Pandas** | 2.1.0 | TransformaciÃ³n de datos |
| **PySpark** | 3.5.0 | Procesamiento distribuido |
| **PostgreSQL** | 13+ | Base de datos relacional |
| **SQLAlchemy** | 2.0.20 | ORM y conexiÃ³n a DB |
| **PyArrow** | 13.0.0 | Formato Parquet |
| **Git** | 2.x | Control de versiones |

---

## ğŸ— Arquitectura

El proyecto sigue una arquitectura modular de tres capas:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTRACCIÃ“N (Extract)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   CSV Files  â”‚â”€â”€â”€â”€â–¶â”‚DataExtractor â”‚â”€â”€â”€â”€â–¶ Parquet    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                TRANSFORMACIÃ“N (Transform)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚DataTransformer â”‚â”€â–¶ Limpieza                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   ValidaciÃ³n                       â”‚
â”‚                       Joins                             â”‚
â”‚                       Agregaciones                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CARGA (Load)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚  â”‚DataLoader  â”‚â”€â–¶ PostgreSQL                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   CSV Reports                          â”‚
â”‚                   Excel Reports                         â”‚
â”‚                   Parquet Files                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principales

**1. DataExtractor** (`src/extractors/data_extractor.py`)
- Lectura de archivos CSV con Pandas y Spark
- CreaciÃ³n y gestiÃ³n de Spark Sessions
- Lectura/escritura de archivos Parquet
- Carga masiva de tablas Olist

**2. DataTransformer** (`src/transformers/data_transformer.py`)
- Limpieza de datos (nombres de columnas, duplicados)
- ConversiÃ³n de tipos de datos
- Manejo de valores nulos (mÃºltiples estrategias)
- Joins entre tablas relacionales
- CreaciÃ³n de columnas derivadas
- Agregaciones con Spark

**3. DataLoader** (`src/loaders/data_loader.py`)
- ConexiÃ³n a PostgreSQL con SQLAlchemy
- Carga de datos a base de datos
- ExportaciÃ³n a mÃºltiples formatos (CSV, Excel, Parquet)
- GeneraciÃ³n de reportes de calidad de datos
- EjecuciÃ³n de queries SQL

**4. OlistPipeline** (`main.py`)
- OrquestaciÃ³n del flujo ETL completo
- Logging de operaciones
- Manejo de errores y excepciones
- Resumen de ejecuciÃ³n

---

## ğŸ“ Estructura del Proyecto

```
olist-data-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # CSVs originales de Olist
â”‚   â”œâ”€â”€ processed/              # Archivos Parquet procesados
â”‚   â””â”€â”€ reports/                # Reportes generados
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ data_extractor.py  # Clase para extracciÃ³n
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ data_transformer.py # Clase para transformaciÃ³n
â”‚   â””â”€â”€ loaders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ data_loader.py     # Clase para carga
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ queries.sql            # 12 queries de anÃ¡lisis
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ main.py                    # Orquestador principal
```

---

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.9 o superior
- PostgreSQL 13+ (opcional, para carga a DB)
- Git

### Pasos de InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone https://github.com/tu-usuario/olist-data-pipeline.git
cd olist-data-pipeline
```

2. **Crear entorno virtual**
```bash
python -m venv venv

# Linux/Mac
source venv/bin/activate

# Windows
venv\Scripts\activate
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Descargar dataset de Olist**
- Descarga desde [Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
- Extrae los CSVs en la carpeta `data/raw/`

5. **Configurar PostgreSQL (opcional)**
```bash
# Edita main.py y configura tu connection string
DB_CONNECTION = "postgresql://user:password@localhost:5432/olist_db"
```

---

## ğŸ’» Uso

### EjecuciÃ³n BÃ¡sica

```bash
python main.py
```

Esto ejecutarÃ¡ el pipeline completo:
- âœ… Extrae datos de los CSVs
- âœ… Aplica transformaciones y limpieza
- âœ… Genera reportes en CSV
- âœ… Muestra resumen de ejecuciÃ³n

### ConfiguraciÃ³n Avanzada

Edita las variables en `main.py`:

```python
# Usar Spark para procesamiento
USE_SPARK = True

# Formato de exportaciÃ³n: 'csv', 'excel', 'parquet', 'all'
EXPORT_FORMAT = 'all'

# Cargar a PostgreSQL
LOAD_TO_DB = True
DB_CONNECTION = "postgresql://user:pass@localhost:5432/db"
```

### Uso Modular

Puedes usar cada clase de forma independiente:

```python
from src.extractors.data_extractor import DataExtractor
from src.transformers.data_transformer import DataTransformer

# Extraer datos
extractor = DataExtractor()
tables = extractor.load_all_olist_tables()

# Transformar
transformer = DataTransformer()
orders_clean = transformer.add_derived_columns_orders(tables['orders'])
```

---

## âš¡ CaracterÃ­sticas Principales

### 1. ExtracciÃ³n de Datos
- âœ… Lectura eficiente de mÃºltiples CSVs
- âœ… Soporte para Pandas y PySpark
- âœ… ConversiÃ³n automÃ¡tica a formato Parquet
- âœ… Manejo de errores y archivos faltantes

### 2. TransformaciÃ³n
- âœ… Limpieza automatizada de datos
- âœ… Manejo inteligente de valores nulos (4 estrategias)
- âœ… DetecciÃ³n y eliminaciÃ³n de duplicados
- âœ… Joins relacionales entre 8+ tablas
- âœ… CreaciÃ³n de mÃ©tricas derivadas:
  - Tiempo de entrega
  - Retrasos en envÃ­os
  - AnÃ¡lisis temporal
- âœ… Logging completo de transformaciones

### 3. Carga y ExportaciÃ³n
- âœ… Carga masiva a PostgreSQL
- âœ… ExportaciÃ³n multi-formato (CSV, Excel, Parquet)
- âœ… GeneraciÃ³n de reportes de calidad de datos
- âœ… EjecuciÃ³n de queries personalizadas

### 4. Buenas PrÃ¡cticas
- âœ… Arquitectura POO modular y extensible
- âœ… Type hints en Python
- âœ… Docstrings en todas las funciones
- âœ… Manejo robusto de excepciones
- âœ… Logging de operaciones
- âœ… CÃ³digo reutilizable y testeable

---

## ğŸ“Š Queries SQL

El proyecto incluye **12 queries SQL avanzadas** en `sql/queries.sql`:

| # | Query | Conceptos Demostrados |
|---|-------|----------------------|
| 1 | Ventas por CategorÃ­a | JOIN, GROUP BY, agregaciones |
| 2 | Top Vendedores | JOIN mÃºltiple, subqueries |
| 3 | AnÃ¡lisis de Entregas | Date functions, CASE WHEN |
| 4 | Clientes por Estado | JOIN, agregaciones mÃºltiples |
| 5 | Productos MÃ¡s Vendidos | Window functions, PARTITION BY |
| 6 | MÃ©todos de Pago | GROUP BY, ORDER BY |
| 7 | Ventas Temporales | Date functions, GROUP BY temporal |
| 8 | AnÃ¡lisis de Reviews | JOIN, agregaciones condicionales |
| 9 | Clientes Valiosos | Subqueries, ORDER BY |
| 10 | Costos de EnvÃ­o | JOIN, agregaciones, HAVING |
| 11 | Cohort Analysis | CTEs, Window functions complejas |
| 12 | Dashboard Summary | MÃºltiples CTEs, mÃ©tricas generales |

### Ejemplo de Query

```sql
-- Top 10 Vendedores por Revenue
SELECT 
    s.seller_id,
    s.seller_city,
    COUNT(DISTINCT oi.order_id) AS ordenes,
    ROUND(SUM(oi.price)::NUMERIC, 2) AS revenue_total,
    ROUND(AVG(r.review_score)::NUMERIC, 2) AS rating
FROM sellers s
INNER JOIN order_items oi ON s.seller_id = oi.seller_id
INNER JOIN orders o ON oi.order_id = o.order_id
LEFT JOIN reviews r ON o.order_id = r.order_id
WHERE o.order_status = 'delivered'
GROUP BY s.seller_id, s.seller_city
ORDER BY revenue_total DESC
LIMIT 10;
```

---

## ğŸ“ˆ Resultados

### MÃ©tricas Procesadas

- **100,000+** Ã³rdenes analizadas
- **8 tablas** relacionales procesadas
- **50+ columnas** derivadas creadas
- **12 reportes** analÃ­ticos generados

### Insights Obtenidos

1. **CategorÃ­as mÃ¡s vendidas**: ElectrÃ³nicos, muebles, deportes
2. **Tiempo promedio de entrega**: 12-15 dÃ­as
3. **Tasa de entregas tarde**: ~10%
4. **Rating promedio**: 4.2/5.0
5. **MÃ©todos de pago**: Credit card (76%), Boleto (19%)

---

## ğŸ”® PrÃ³ximas Mejoras

- [ ] Implementar tests unitarios con Pytest
- [ ] Agregar pipeline CI/CD con GitHub Actions
- [ ] Dockerizar la aplicaciÃ³n
- [ ] Implementar Apache Airflow para scheduling
- [ ] Agregar dashboard con Streamlit/Dash
- [ ] Integrar con AWS S3 y Redshift
- [ ] Implementar data quality checks automÃ¡ticos
- [ ] Agregar monitoring con Grafana

---

## ğŸ‘¨â€ğŸ’» Autor

**[Roberto Gomez]**

- ğŸ“§ Email: roberto.kgc@gmail.com 
- ğŸ’¼ LinkedIn: www.linkedin.com/in/rkgc0897
- ğŸ™ GitHub: https://github.com/RobertoGC97 

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ™ Agradecimientos

- Dataset proporcionado por [Olist](https://olist.com/) en Kaggle
- InspiraciÃ³n de proyectos de la comunidad de Data Engineering
- DocumentaciÃ³n oficial de PySpark, Pandas y PostgreSQL

---

## ğŸ“š Referencias

- [Dataset Original - Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
